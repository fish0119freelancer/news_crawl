# main.py
# RSS + é—œéµå­—ç¯©é¸(å¯é–‹é—œ) + é ˜åŸŸåˆ†çµ„ + æ¯é ˜åŸŸæœ€å¤š5ç¯‡ + LLM æ‘˜è¦ + PDF
import os
import re
from datetime import datetime
from pathlib import Path

from summarize_with_llm import generate_news_summary_and_opinion, llm_batch_summarize
from report_generator import format_report
from fetch_articles import fetch_today_from_rss
from generate_pdf_summary import md_to_pdf

# ====== FLAGï¼šæ˜¯å¦å•Ÿç”¨é—œéµå­—ç¯©é¸ ======
USE_KEYWORDS = True   # â† True å•Ÿç”¨é—œéµå­—ç¯©é¸ï¼ŒFalse å…¨éƒ¨æ–‡ç« éƒ½æœƒè™•ç†

# ====== FLAGï¼šæ¯å€‹é ˜åŸŸæœ€å¤šè™•ç†å¹¾ç¯‡ ======
MAX_PER_DOMAIN = 5

# ====== é—œéµå­—è¨­å®šï¼ˆå¯ç”¨ keywords.txt è¦†è“‹ï¼‰ ======
DEFAULT_KEYWORDS = [
    # ç”Ÿç†è¨Šè™Ÿ / é†«ç™‚è£ç½®
    "biomedical signal", "biosignal", "ECG", "EEG", "EMG", "PPG", "rPPG", "BCG", "stethoscope", "heart sound",
    "wearable", "wearable device", "smart wearable", "medical device", "biosensor", "sensor fusion",

    # å¤–æ³Œé«” / ç²¾æº–é†«ç™‚
    "extracellular vesicle", "extracellular vesicles", "exosome", "exosomes",
    "liquid biopsy", "circulating nucleic acid", "circulating tumor DNA", "ctDNA",

    # ç¥ç¶“ç§‘å­¸ / å¿ƒç†å­¸
    "neuroscience", "brain", "brain-computer interface", "BCI", "neurotechnology",
    "cognitive neuroscience", "neuroimaging", "EEG-based", "fMRI", "psychology", "behavioral science",
    "mental health", "psychiatry",

    # AI / æ•¸æ“šåˆ†æ
    "artificial intelligence", "machine learning", "deep learning", "computer vision", "medical imaging",
    "medical AI", "signal processing", "digital health", "telemedicine", "remote monitoring",

    # ç”¢æ¥­è¶¨å‹¢ / æ³•è¦
    "medtech", "healthtech", "biotech", "precision medicine", "regulatory", "FDA", "MDR", "TFDA",
    "CE mark", "market analysis",

    # ç”Ÿç§‘åŸºç¤ç ”ç©¶
    "cell biology", "molecular biology", "genetics", "genomics", "proteomics", "transcriptomics",
    "epigenetics", "immunology", "stem cell", "cancer biology", "developmental biology", "metabolism", "biochemistry",
]

KEYWORDS_FILE = "keywords.txt"
KEYWORD_MODE = "OR"  # OR / AND

def load_keywords(path: str) -> list[str]:
    if Path(path).exists():
        with open(path, "r", encoding="utf-8") as f:
            kws = [ln.strip() for ln in f if ln.strip() and not ln.strip().startswith("#")]
        if kws:
            return kws
    return DEFAULT_KEYWORDS

KEYWORDS = load_keywords(KEYWORDS_FILE)

def _word_boundary_pattern(kw: str) -> re.Pattern:
    if re.fullmatch(r"[A-Za-z0-9\-\+_/\.]+", kw):
        return re.compile(rf"\b{re.escape(kw)}\b", flags=re.IGNORECASE)
    return re.compile(re.escape(kw), flags=re.IGNORECASE)

KW_PATTERNS = [(_word_boundary_pattern(kw), kw) for kw in KEYWORDS]

def keyword_hits(text: str) -> list[str]:
    hits = []
    for pat, raw in KW_PATTERNS:
        if pat.search(text):
            hits.append(raw)
    return hits

def article_match(article: dict) -> tuple[bool, list[str]]:
    """æª¢æŸ¥æ–‡ç« æ˜¯å¦ç¬¦åˆé—œéµå­—"""
    if not USE_KEYWORDS:
        return True, []
    text = " ".join([
        article.get("title", "") or "",
        article.get("summary", "") or "",
        article.get("text", "") or "",
        " ".join(article.get("categories", []) or [])
    ])
    hits = keyword_hits(text)
    if KEYWORD_MODE == "AND":
        ok = all(any(h.lower() == kw.lower() for h in hits) for kw in KEYWORDS)
    else:
        ok = len(hits) > 0
    return ok, hits

# ====== é ˜åŸŸåˆ†çµ„ï¼ˆæ¯çµ„æœ€å¤š MAX_PER_DOMAIN ç¯‡ï¼‰ ======
DOMAIN_MAP = {
    "signal": [
        "biomedical signal", "biosignal", "ECG", "EEG", "EMG", "PPG", "rPPG", "BCG",
        "stethoscope", "heart sound", "wearable", "wearable device", "smart wearable",
        "medical device", "biosensor", "sensor fusion"
    ],
    "extracellular": [
        "extracellular vesicle", "extracellular vesicles", "exosome", "exosomes",
        "liquid biopsy", "circulating nucleic acid", "circulating tumor DNA", "ctDNA"
    ],
    "neuro": [
        "neuroscience", "brain", "brain-computer interface", "BCI", "neurotechnology",
        "cognitive neuroscience", "neuroimaging", "EEG-based", "fMRI",
        "psychology", "behavioral science", "mental health", "psychiatry"
    ],
    "ai": [
        "artificial intelligence", "machine learning", "deep learning", "computer vision",
        "medical imaging", "medical AI", "signal processing", "digital health",
        "telemedicine", "remote monitoring"
    ],
    "industry": [
        "medtech", "healthtech", "biotech", "precision medicine", "regulatory",
        "FDA", "MDR", "TFDA", "CE mark", "market analysis"
    ],
    "basicbio": [
        "cell biology", "molecular biology", "genetics", "genomics", "proteomics",
        "transcriptomics", "epigenetics", "immunology", "stem cell", "cancer biology",
        "developmental biology", "metabolism", "biochemistry"
    ]
}

def classify_domain(text: str) -> str:
    t = text.lower()
    for domain, kws in DOMAIN_MAP.items():
        if any(k.lower() in t for k in kws):
            return domain
    return "other"

domain_count = {d: 0 for d in DOMAIN_MAP}

# ===== åˆå§‹åŒ–æª”æ¡ˆèˆ‡æ—¥æœŸ =====
today_str = datetime.today().strftime("%Y%m%d")
md_filename = f"news_report_{today_str}.md"
pdf_filename = f"news_summary_{today_str}.pdf"
Path(md_filename).unlink(missing_ok=True)

# ===== è®€å– RSS URL =====
with open("urls.txt", "r", encoding="utf-8") as f:
    urls = [line.strip() for line in f if line.strip()]

total_sources = len(urls)
success_count = 0
fail_count = 0
success_sources = 0
skipped_by_keyword = 0

print(f"ğŸ” å…± {total_sources} å€‹ä¾†æºç¶²ç«™ï¼Œé–‹å§‹æƒæä»Šå¤©çš„æ–°æ–‡ç« ...")
if USE_KEYWORDS:
    print(f"ğŸ§² é—œéµå­—ç¯©é¸ï¼šå·²å•Ÿç”¨ ({KEYWORD_MODE})ï¼Œé—œéµå­—æ•¸é‡ï¼š{len(KEYWORDS)}")
else:
    print("ğŸ§² é—œéµå­—ç¯©é¸ï¼šå·²åœç”¨ï¼Œæ‰€æœ‰æ–‡ç« éƒ½æœƒè™•ç†")
print(f"âš–ï¸ æ¯å€‹é ˜åŸŸæœ€å¤šè™•ç† {MAX_PER_DOMAIN} ç¯‡æ–‡ç« ")

# ===== ä¸»è¿´åœˆ =====
for idx, url in enumerate(urls, 1):
    try:
        print(f"\nğŸ“¡ [{idx}/{total_sources}] æƒæä¾†æºï¼š{url}")
        today_articles = fetch_today_from_rss(url)
        if not today_articles:
            print(f"âš ï¸ ä»Šæ—¥ç„¡æ–°æ–‡ç« ï¼š{url}")
            continue

        print(f"ğŸ“° ç™¼ç¾ {len(today_articles)} ç¯‡æ–°æ–‡ç« ")
        source_success = 0

        for i, article in enumerate(today_articles, 1):
            try:
                if "text" not in article:
                    article["text"] = article.get("summary", "")

                ok, hits = article_match(article)
                if not ok:
                    skipped_by_keyword += 1
                    print(f"  â­ï¸ é—œéµå­—æœªå‘½ä¸­ï¼š{article.get('title','(ç„¡æ¨™é¡Œ)')}")
                    continue

                # åˆ†é¡é ˜åŸŸä¸¦æª¢æŸ¥æ•¸é‡é™åˆ¶
                fulltext = " ".join([article.get("title",""), article.get("summary",""), article.get("text","")])
                domain = classify_domain(fulltext)
                if domain in domain_count and domain_count[domain] >= MAX_PER_DOMAIN:
                    print(f"  ğŸš« {article.get('title')} å·²é” {domain} ä¸Šé™ {MAX_PER_DOMAIN}")
                    continue

                print(f"  â³ [{i}/{len(today_articles)}] è™•ç†æ–‡ç« ï¼š{article['title']} ï½œğŸ¯ å‘½ä¸­ï¼š{', '.join(hits[:6])}{'â€¦' if len(hits) > 6 else ''}")
                summary_and_opinion = generate_news_summary_and_opinion(article)
                report = format_report(article, summary_and_opinion)

                with open(md_filename, "a", encoding="utf-8") as f:
                    f.write(report + "\n\n" + "-"*90 + "\n\n")

                success_count += 1
                source_success += 1
                if domain in domain_count:
                    domain_count[domain] += 1

            except Exception as article_err:
                print(f"  âŒ æ–‡ç« è™•ç†å¤±æ•—ï¼š{article.get('title','(ç„¡æ¨™é¡Œ)')} â†’ {article_err}")
                fail_count += 1

        if source_success > 0:
            success_sources += 1

    except Exception as source_err:
        print(f"âŒ ä¾†æºè™•ç†å¤±æ•—ï¼š{url} â†’ {source_err}")
        fail_count += 1

# ===== çµ±è¨ˆå ±å‘Š =====
print("\nğŸ“Š çˆ¬èŸ²å®Œæˆ")
print(f"âœ”ï¸ æˆåŠŸè™•ç†æ–‡ç« æ•¸ï¼š{success_count}")
if USE_KEYWORDS:
    print(f"â¤´ï¸ é—œéµå­—æœªå‘½ä¸­è€Œç•¥éï¼š{skipped_by_keyword}")
print(f"âŒ å¤±æ•—æ–‡ç« æ•¸ï¼š{fail_count}")
print(f"ğŸ“„ æˆåŠŸä¾†æºç¸½æ•¸ï¼š{success_sources}ï¼{total_sources}")

# ===== ç”¢å‡º PDF =====
md_to_pdf(md_filename, pdf_filename)
print(f"âœ… PDF å·²å®Œæˆï¼š{pdf_filename}")
