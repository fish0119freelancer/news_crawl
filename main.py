# main.py (RSS ç‰ˆ + é—œéµå­—ç¯©é¸å¯é–‹é—œ + LLM æ‘˜è¦ + PDF)
import os
import re
from datetime import datetime
from pathlib import Path

from summarize_with_llm import generate_news_summary_and_opinion, llm_batch_summarize
from report_generator import format_report
from fetch_articles import fetch_today_from_rss
from generate_pdf_summary import md_to_pdf

# ====== FLAGï¼šæ˜¯å¦å•Ÿç”¨é—œéµå­—ç¯©é¸ ======
USE_KEYWORDS = False   # â† ç›´æ¥æ”¹é€™å€‹ True / False å°±èƒ½åˆ‡æ›

# ====== é—œéµå­—è¨­å®šï¼ˆå¯ç”¨ keywords.txt è¦†è“‹ï¼‰ ======
DEFAULT_KEYWORDS = [
    "æ™ºæ…§åº§è‰™", "smart cockpit", "smart cabin", "åº§è‰™", "cockpit",
    "infotainment", "IVI", "HMI", "è½¦æœº", "è»Šæ©Ÿ", "ä¸­æ§", "èªéŸ³åŠ©ç†", "è»Šè¼‰èªéŸ³",
    "AR-HUD", "HUD", "æŠ¬é ­é¡¯ç¤º", "å„€è¡¨æ¿", "ä¸­æ§å±", "å¤šå±",
    "è»Šç”¨", "automotive", "in-vehicle", "domain controller", "è»Šè¦", "SoC", "MCU",
    "ä»¥å¤ªç¶²", "Ethernet", "LIN", "CAN", "CAN-FD", "MOST",
    "ADAS", "è‡ªå‹•é§•é§›", "autonomous", "æ„Ÿæ¸¬å™¨", "é›·é”", "LiDAR", "æ¯«ç±³æ³¢", "è¶…è²æ³¢",
    "æ”å½±æ©Ÿ", "camera", "èåˆ", "sensor fusion",
    "è»Šè¯ç¶²", "V2X", "5G", "OTA", "FOTA", "SOTA", "SDV", "software-defined",
    "åŠŸèƒ½å®‰å…¨", "ISO 26262", "ASIL", "cybersecurity", "UN R155", "UN R156",
    "Qualcomm", "Snapdragon", "Cockpit", "NVIDIA", "Orin", "Drive", "Horizon Robotics",
    "åœ°å¹³ç·š", "Ambarella", "Black Sesame", "é»‘èŠéº»", "ç‘è–©", "Renesas",
    "NXP", "TI", "Infineon", "MediaTek", "è¯ç™¼ç§‘", "Samsung", "Intel", "AMD",
]

KEYWORDS_FILE = "keywords.txt"
KEYWORD_MODE = "OR"  # OR / AND


def load_keywords(path: str) -> list[str]:
    if Path(path).exists():
        with open(path, "r", encoding="utf-8") as f:
            kws = [ln.strip() for ln in f if ln.strip() and not ln.strip().startswith("#")]
        if kws:
            return kws
    return DEFAULT_KEYWORDS


KEYWORDS = load_keywords(KEYWORDS_FILE)


def _word_boundary_pattern(kw: str) -> re.Pattern:
    if re.fullmatch(r"[A-Za-z0-9\-\+_/\.]+", kw):
        return re.compile(rf"\b{re.escape(kw)}\b", flags=re.IGNORECASE)
    return re.compile(re.escape(kw), flags=re.IGNORECASE)


KW_PATTERNS = [(_word_boundary_pattern(kw), kw) for kw in KEYWORDS]


def keyword_hits(text: str) -> list[str]:
    hits = []
    for pat, raw in KW_PATTERNS:
        if pat.search(text):
            hits.append(raw)
    return hits


def article_match(article: dict) -> tuple[bool, list[str]]:
    """æª¢æŸ¥æ–‡ç« æ˜¯å¦ç¬¦åˆé—œéµå­—"""
    if not USE_KEYWORDS:  # â† ä¸å•Ÿç”¨ç¯©é¸æ™‚ç›´æ¥å…¨éƒ¨é€šé
        return True, []

    title = article.get("title", "") or ""
    desc = article.get("summary", "") or ""
    body = article.get("text", "") or ""
    cats = " ".join(article.get("categories", []) or [])
    blob = " ".join([title, desc, body, cats]).lower()

    hits = keyword_hits(blob)
    if KEYWORD_MODE == "AND":
        ok = all(any(h.lower() == kw.lower() for h in hits) for kw in KEYWORDS)
    else:
        ok = len(hits) > 0
    return ok, hits


# ===== åˆå§‹åŒ–æª”æ¡ˆèˆ‡æ—¥æœŸ =====
today_str = datetime.today().strftime("%Y%m%d")
md_filename = f"news_report_{today_str}.md"
pdf_filename = f"news_summary_{today_str}.pdf"

Path(md_filename).unlink(missing_ok=True)

# ===== è®€å– RSS URL =====
with open("urls.txt", "r", encoding="utf-8") as f:
    urls = [line.strip() for line in f if line.strip()]

total_sources = len(urls)
success_count = 0
fail_count = 0
success_sources = 0
skipped_by_keyword = 0

print(f"ğŸ” å…± {total_sources} å€‹ä¾†æºç¶²ç«™ï¼Œé–‹å§‹æƒæä»Šå¤©çš„æ–°æ–‡ç« ...")
if USE_KEYWORDS:
    print(f"ğŸ§² é—œéµå­—ç¯©é¸ï¼šå·²å•Ÿç”¨ ({KEYWORD_MODE})ï¼Œé—œéµå­—æ•¸é‡ï¼š{len(KEYWORDS)}")
else:
    print("ğŸ§² é—œéµå­—ç¯©é¸ï¼šå·²åœç”¨ï¼Œæ‰€æœ‰æ–‡ç« éƒ½æœƒè™•ç†")

# ===== ä¸»è¿´åœˆ =====
for idx, url in enumerate(urls, 1):
    try:
        print(f"\nğŸ“¡ [{idx}/{total_sources}] æƒæä¾†æºï¼š{url}")
        today_articles = fetch_today_from_rss(url)

        if not today_articles:
            print(f"âš ï¸ ä»Šæ—¥ç„¡æ–°æ–‡ç« ï¼š{url}")
            continue

        print(f"ğŸ“° ç™¼ç¾ {len(today_articles)} ç¯‡æ–°æ–‡ç« ")
        source_success = 0

        for i, article in enumerate(today_articles, 1):
            try:
                if "text" not in article:
                    article["text"] = article.get("summary", "")

                ok, hits = article_match(article)
                if not ok:
                    skipped_by_keyword += 1
                    print(f"  â­ï¸ é—œéµå­—æœªå‘½ä¸­ï¼š{article.get('title','(ç„¡æ¨™é¡Œ)')}")
                    continue

                print(f"  â³ [{i}/{len(today_articles)}] è™•ç†æ–‡ç« ï¼š{article['title']} ï½œğŸ¯ å‘½ä¸­ï¼š{', '.join(hits[:6])}{'â€¦' if len(hits) > 6 else ''}")

                summary_and_opinion = generate_news_summary_and_opinion(article)
                report = format_report(article, summary_and_opinion)

                with open(md_filename, "a", encoding="utf-8") as f:
                    f.write(report + "\n\n-----------------------------------------------------------------------------------------\n\n")

                print(f"  âœ… æˆåŠŸï¼š{article['title']}")
                success_count += 1
                source_success += 1

            except Exception as article_err:
                print(f"  âŒ æ–‡ç« è™•ç†å¤±æ•—ï¼š{article.get('title','(ç„¡æ¨™é¡Œ)')} â†’ {article_err}")
                fail_count += 1

        if source_success > 0:
            success_sources += 1

    except Exception as source_err:
        print(f"âŒ ä¾†æºè™•ç†å¤±æ•—ï¼š{url} â†’ {source_err}")
        fail_count += 1

# ===== çµ±è¨ˆå ±å‘Š =====
print("\nğŸ“Š çˆ¬èŸ²å®Œæˆ")
print(f"âœ”ï¸ æˆåŠŸè™•ç†æ–‡ç« æ•¸ï¼š{success_count}")
if USE_KEYWORDS:
    print(f"â¤´ï¸ é—œéµå­—æœªå‘½ä¸­è€Œç•¥éï¼š{skipped_by_keyword}")
print(f"âŒ å¤±æ•—æ–‡ç« æ•¸ï¼š{fail_count}")
print(f"ğŸ“„ æˆåŠŸä¾†æºç¸½æ•¸ï¼š{success_sources}ï¼{total_sources}")

# ===== ç”¢å‡º PDF =====
md_to_pdf(md_filename, pdf_filename)
print(f"âœ… PDF å·²å®Œæˆï¼š{pdf_filename}")
