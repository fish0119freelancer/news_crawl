from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# å–®ç¯‡æ–‡ç« è™•ç†
def generate_news_summary_and_opinion(article):
    prompt = f"""
ä½ æ˜¯ä¸€ä½ã€Œç”Ÿé†«è·¨é ˜åŸŸæå€¡è€…ã€ï¼Œéœ€è¦å¹«åŠ©è®€è€…å¿«é€Ÿç†è§£æœ€æ–°ç”Ÿé†«/é†«å·¥æ–‡ç« ã€‚

è«‹é‡å°ä»¥ä¸‹æ–‡ç« è³‡è¨Šï¼Œç”Ÿæˆä¸€å‰‡æ–°èè§£è®€ï¼Œä¸¦åš´æ ¼åŒ…å«äº”å€‹éƒ¨åˆ†ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
# æ¨™é¡Œ
(é ˆå°‡åŸæ–‡æ¨™é¡Œè½‰ç‚ºç¹é«”ä¸­æ–‡)

## æ‘˜è¦
(ä»¥åŸä¾†æ–‡ç« å…§å®¹çš„æ–¹å¼åšç¿»è­¯ï¼Œé©ç•¶åˆ†æ®µï¼Œä¸¦ä¿ç•™å°ˆæœ‰åè©)

## å°è®€
ï¼ˆä»¥åˆå­¸è€…è§’åº¦ï¼Œæ·ºç™½è§£é‡‹ï¼Œä½†ä¿ç•™å°ˆæœ‰åè©ï¼Œä¸¦åœ¨æ‹¬è™Ÿå…§è£œå……ç°¡å–®å®šç¾©ï¼‰

## å­¸ç¿’è·¯å¾‘
è«‹ç”¨ã€ŒA â†’ B â†’ C â†’ â€¦ã€çš„ç®­é ­æ ¼å¼ï¼Œè¨­è¨ˆå‡ºä¸€æ¢å¾åŸºç¤åˆ°é€²éšçš„çŸ¥è­˜å­¸ç¿’é †åºã€‚
æ¯å€‹ç¯€é»æ‡‰è©²æ˜¯å¯æŸ¥è©¢çš„é—œéµè©æˆ–å­¸ç§‘é ˜åŸŸï¼Œå¹«åŠ©è®€è€…èƒ½è‡ªå­¸æ‰¾åˆ°è³‡æºã€‚

## åŸæ–‡é€£çµ
[é»æ“Šé€™è£¡é–±è®€åŸæ–‡]({article.get('url', '')})

---
æ–‡ç« æ¨™é¡Œï¼š{article['title']}
æ–‡ç« å…§å®¹ï¼š{article['text']}
æ–‡ç« ç¶²å€ï¼š{article.get('url', '')}
"""

    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4
    )
    return res.choices[0].message.content.strip()


# æ‰¹æ¬¡è™•ç†ï¼šé¿å…çˆ† token
def llm_batch_summarize(paragraphs, batch_size=1):
    """
    å°‡å¤šç¯‡æ–‡ç« é€ä¸€æ‘˜è¦ â†’ åˆä½µæˆç¸½çµ
    paragraphs: list[str] æˆ– list[dict]
    """
    summaries = []

    for i, para in enumerate(paragraphs, 1):
        # å¦‚æœå‚³é€²ä¾†æ˜¯ dictï¼Œå– text
        text = para["text"] if isinstance(para, dict) else para
        text = text[:4000]  # å®‰å…¨åˆ‡æ–·ï¼Œé¿å…å–®ç¯‡éé•·

        prompt = f"""
ä½ æ˜¯ä¸€ä½ã€Œç”Ÿé†«è·¨é ˜åŸŸæå€¡è€…ã€ï¼Œè«‹å¹«æˆ‘æ¿ƒç¸®ä»¥ä¸‹æ–‡ç« é‡é»ï¼Œè¼¸å‡º 1 æ®µã€Œç²¾ç°¡æ‘˜è¦ã€å³å¯ï¼š
{text}
"""
        res = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        summary = res.choices[0].message.content.strip()
        summaries.append(summary)

    # æœ€å¾Œåˆä½µæ‰€æœ‰æ‘˜è¦ï¼Œå†ç”¢å‡ºç¸½çµ
    combined = "\n\n".join(summaries)
    final_prompt = f"""
ä»¥ä¸‹æ˜¯å¤šç¯‡ç”Ÿé†«æ–‡ç« çš„ç²¾ç°¡æ‘˜è¦ï¼Œè«‹æ•´åˆæˆä¸€ä»½ã€Œæ¯æ—¥ç”Ÿé†«æ–°èå ±å‘Šã€ï¼Œ
ä¸¦ç¶­æŒ Markdown æ ¼å¼ï¼Œç‚ºæ¯ç¯‡æ–‡ç« ç”Ÿæˆï¼š

1. æ¨™é¡Œ  
2. æ‘˜è¦  
3. å°è®€ï¼ˆæ·ºç™½è§£è®€ï¼‰  
4. å­¸ç¿’è·¯å¾‘ï¼ˆçŸ¥è­˜åœ°åœ–ï¼‰  
5. åŸæ–‡é€£çµ  

ä»¥ä¸‹æ˜¯å¤šç¯‡æ‘˜è¦ï¼š
{combined}
"""

    res = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": final_prompt}],
        temperature=0.4
    )

    return res.choices[0].message.content.strip()

if __name__ == "__main__":
    from generate_pdf_summary import extract_references_from_md, generate_pdf

    # è®€å–æ¸…ç†å¾Œçš„æ–‡ç« 
    cleaned_paragraphs, references = extract_references_from_md("news_report.md")

    # é¿å…ä¸€æ¬¡çˆ†æ‰ï¼Œæ”¹æˆåˆ†æ‰¹è™•ç†
    # print(f"ğŸ” å…± {len(cleaned_paragraphs)} ç¯‡æ–‡ç« ï¼Œé–‹å§‹åˆ†æ‰¹æ‘˜è¦...")
    # final_summary = llm_batch_summarize(cleaned_paragraphs)

    # è¼¸å‡º PDF
    generate_pdf(cleaned_paragraphs, references)
    print("âœ… PDF å·²å®Œæˆï¼šnews_summary.pdf")
